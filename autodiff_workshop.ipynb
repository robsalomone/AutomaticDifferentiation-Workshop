{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation in Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Autograd\n",
    "**Advantages:** Extremely simple, full-featured. \n",
    "\n",
    "**Disadvantages:** No GPU support, slower than PyTorch, some quirks we will look at shortly. \n",
    "\n",
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.        , 1.41614684])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import autograd \n",
    "import autograd.numpy as np # instead of import numpy as np \n",
    "\n",
    "def f1(x): \n",
    "    return np.log(x[0]) + x[0]*x[1] - np.sin(x[1])\n",
    "\n",
    "grad_f = autograd.grad(f1,0) \n",
    "x = np.array([1.,2.])\n",
    "grad_f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use finite differences to make sure it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD:[3.25      4.9899925], AD:[3.25      4.9899925]\n"
     ]
    }
   ],
   "source": [
    "def central_difference(x,func, h = 1e-5): \n",
    "    n = x.shape[0]\n",
    "    gr = np.zeros(n)\n",
    "        \n",
    "    for i in range(n):\n",
    "        delta = np.zeros(n)\n",
    "        delta[i] += h\n",
    "        xb, xf = x - delta, x + delta\n",
    "        gr[i] = (func(xf) - func(xb))/(2*h)\n",
    "     \n",
    "    return gr \n",
    "    \n",
    "x = np.array([4.,3.]) \n",
    "print(f\"FD:{central_difference(x, func=f1)}, AD:{grad_f(x)}\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd: Two Most Common Pitfalls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Float Arrays\n",
    "\n",
    "Make sure you don't accidently try to autodiff using integers, or other objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.25     , 4.9899925])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([4,3.]) # an integer array \n",
    "grad_f(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array Assignment\n",
    "The following is a perfectly fine function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f2(x): 16.00\n"
     ]
    }
   ],
   "source": [
    "def f2(x):\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    vals = np.zeros(n) \n",
    "    for i in range(n):\n",
    "        vals[i] =  A[i,:] @ x.reshape(-1,1) # ***\n",
    "        \n",
    "    return np.sum(vals)\n",
    "\n",
    "A = np.array([[1,2], [3,4]])\n",
    "x = np.array([1.,2.])\n",
    "\n",
    "# f-strings!\n",
    "print(f\"f2(x): {f2(x) :.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, autograd cannot deal with *array assignment* (line with # ***). It will create a gradient function object just fine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradf2 = autograd.grad(f2,0) # creates a function object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but then when we go to do reverse mode it will throw this will throw a horrible error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d0c6ba581fe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgradf2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/differential_operators.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0marguments\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     should be scalar-valued. The gradient has the same type as the argument.\"\"\"\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         raise TypeError(\"Grad only applies to real scalar-output functions. \"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/core.py\u001b[0m in \u001b[0;36mmake_vjp\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVJPNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mend_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(start_node, fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mstart_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mend_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_box\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstart_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36munary_f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0msubargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubvals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3a27a2f7108c>\u001b[0m in \u001b[0;36mf2\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ***\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "gradf2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When autograd says you are trying to set an *array element with a sequence*, it generally means you are trying to assign into an array - when it tries to build the graph it can't do it. \n",
    "\n",
    "**Solution:** *You should always try to vectorize first.* However, if you can't, or don't want to waste time, the solution is to use **list comprehensions**. List comprehensions (or any comprehensions) work just similar to how you would write set notation in maths. For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x): -12.34\n",
      "FD:[ 0.39866056 -6.37000628], AD:[ 0.39866056 -6.37000628]\n"
     ]
    }
   ],
   "source": [
    "def f3(x): # f2 with list comprehension\n",
    "    vals =  [A[i,:] @ x.reshape(-1,1) for i in range(A.shape[0])]    \n",
    "    return np.sum(vals)\n",
    "\n",
    "A = np.random.randn(100,2)\n",
    "x = np.array([1.,2.])\n",
    "\n",
    "# f-strings!\n",
    "print(f\"f(x): {f3(x) :.2f}\")\n",
    "\n",
    "gradf3 = autograd.grad(f3,0) # creates a function object\n",
    "print(f\"FD:{central_difference(x, func=f3)}, AD:{gradf3(x)}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for reverse mode we need to evaluate the function first to build the graph, so it would make sense to get the function and its gradient evaluated at $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: -12.341352006387245 , gradf: [ 0.39866056 -6.37000628]\n"
     ]
    }
   ],
   "source": [
    "f3aux = autograd.value_and_grad(f3, 0)\n",
    "val, grad = f3aux(x)\n",
    "\n",
    "print('f:', val, ', gradf:', grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If what we know about autodiff is correct, then ${\\sf gradf3}$ and ${\\sf f3aux}$ should be as fast as each other... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.2 ms ± 1.44 ms per loop (mean ± std. dev. of 25 runs, 5 loops each)\n",
      "14.9 ms ± 1.06 ms per loop (mean ± std. dev. of 25 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 5 -r 25 gradf3(x)\n",
    "%timeit -n 5 -r 25 f3aux(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How embarassing! :D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something a bit more complicated!\n",
    "Let's consider the function\n",
    "\n",
    "$$f(A, c,x,y) = x^\\top \\cdot A\\cdot x+\\cdot \\sin(y)^\\top \\cdot x,$$\n",
    "\n",
    "where $A$ is a matrix,  $c$ is a scalar,  $x$ is a vector, $y$ is a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.52441295]]\n",
      "gA: [[1. 2.]\n",
      " [2. 4.]]\n",
      "gx: [[10.84147098]\n",
      " [ 8.84147098]]\n",
      "gy: [[0.54030231]\n",
      " [1.08060461]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%reset -f\n",
    "import autograd \n",
    "import autograd.numpy as np \n",
    "\n",
    "\n",
    "def f(A, x, y):\n",
    "    return x.T @ A @ x + np.sin(y).T @ x\n",
    "\n",
    "A = np.array([[1.,2.],[2.,1.]])\n",
    "x = np.array([[1.,2.]]).T\n",
    "y = np.array([[1.,1.]]).T\n",
    "\n",
    "val = f(A,x,y)\n",
    "print(val)\n",
    "\n",
    "g_A = autograd.grad(f,0)\n",
    "g_x = autograd.grad(f,1)\n",
    "g_y = autograd.grad(f,2)\n",
    "\n",
    "print('gA:', g_A(A,x,y))\n",
    "print('gx:', g_x(A,x,y))\n",
    "print('gy:', g_y(A,x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this requires 3x forward and backward passes through the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': (2, 2), 'x': (2, 1), 'y': (2, 1)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vec(v):  return np.reshape(v,(-1,1),order=\"F\")\n",
    "def vecinv(v, shape): return np.reshape(v, shape, order=\"F\")\n",
    "def pack(collection): return np.vstack([vec(item) for item in collection])\n",
    "\n",
    "shapes = {'A': A.shape, 'x' : x.shape, 'y' : y.shape}    \n",
    "print(shapes) \n",
    "\n",
    "pack([A,x,y]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, now we a new function that takes in the above vector as an argument, and \"unpacks it\" inside. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessians and Hessian-Vector Products\n",
    "${\\sf Autograd}$ his nice built in support for evaluating full Hessians, and Hessian-Vector Products...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x): # f2 with list comprehension\n",
    "    vals =  [A[i,:] @ np.sin(x.reshape(-1,1)) for i in range(A.shape[0])]    \n",
    "    return np.sum(vals)\n",
    "\n",
    "d = 50000\n",
    "A = np.random.rand(10,d)\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "x = 2*np.ones(d)\n",
    "r = np.random.rand(d)\n",
    "\n",
    "# create a function \n",
    "hvp_naive = lambda x,r : autograd.hessian(f3)(x) @ r\n",
    "\n",
    "# autograd.hessian_vector_product returns a *function* with input (x,r)\n",
    "hvp = autograd.hessian_vector_product(f3,argnum=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is performance is highly non-trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVP...\n",
      "25.5 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print('HVP...')\n",
    "%timeit -n 1 -r 1 hvp(x, r)# hvp(x,r)\n",
    "#print('\\nForm Full Hessian and Multiply..')\n",
    "#%timeit -n 10 -r 10 hvp_naive(x,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# PyTorch\n",
    "**Advantages:** Faster than ${\\sf autograd}$, GPU support, less quirks, probabilistic programming libraries, other modules built on it, lower level control. \n",
    "\n",
    "**Disadvantages:** Reverse mode only, with all documentation in that context, so need to know more autodiff theory to \"get\" it ( :D ), lower level control makes things a bit more involved.  \n",
    "\n",
    "## Basics\n",
    "${\\sf Autograd}$ is ${\\sf numpy}$-native. However, Pytorch has its own ${\\sf Tensor}$ objects, which are used instead of ${\\sf numpy}$ ${\\sf ndarrays}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f \n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "\n",
    "x = torch.tensor([[1.],[2.],[3.]]) # column vector\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, support for the matrix multiplication operator @ and transpose attribute have been built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [2., 4., 6.],\n",
       "       [3., 6., 9.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x @ x.T).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is very nice, because until more recent versions the above code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 4., 6.],\n",
       "        [3., 6., 9.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x, torch.transpose(x,0,1)) # the old way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any ${\\sf Pytorch}$ ${\\sf Tensor}$ object actually has a ${\\sf .numpy}$ function, which casts it as a ${\\sf numpy}$ array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.],\n",
       "       [3.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hang on, **float32**, whats with that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [2.]\n",
      " [3.]] float64\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.],[2.],[3.]], dtype = torch.float64) # column vector\n",
    "print(x.numpy(), x.numpy().dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we are back to double precision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generally**, ${\\sf PyTorch}$ as the same syntax and functions as ${\\sf Numpy}$, with occasionally minor differences: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [2., 4., 6.],\n",
      "        [3., 6., 9.]], dtype=torch.float64)\n",
      "[[1. 2. 3.]\n",
      " [2. 4. 6.]\n",
      " [3. 6. 9.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.matmul(x.numpy(), np.transpose(x.numpy()))\n",
    "B = torch.matmul(x, torch.transpose(x, 0, 1)) # PyTorch insists on you telling it which dimensions to swap \n",
    "\n",
    "print(B)\n",
    "print(A) # numpy ndarrays "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch builds the computation graph in the background by \"watching\" what you do to the tensors. However, you need to tell it to do the watching. This is done with the **requires_gradient** keyword argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1.],[2.],[3.]],\n",
    "                 dtype = torch.float64,\n",
    "                 requires_grad=True)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch will now construct the graph in the background by logging the primal trace of any operations involving the tensor ${\\sf x}$ during evaluation (forward pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.T @ x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.],\n",
       "         [4.],\n",
       "         [6.]], dtype=torch.float64),)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(y,x)\n",
    "#torch.autograd.grad(y,x, grad_outputs = torch.tensor([[1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to be nifty we can create our own \"gradient_only\" function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17.]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x: x.T @ x \n",
    "gr_f = lambda x: torch.autograd.grad(f(x),x) \n",
    "\n",
    "\n",
    "x = torch.tensor([[2],[2],[3.]], requires_grad=True)\n",
    "\n",
    "f(x)\n",
    "#gr_f(torch.tensor([[1],[2],[3]]) # spot the error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So ${\\sf grad}$ is the main function, but it assumes you have created the tensors and executed for forward pass first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients the \"advanced\" way: ${\\sf .backward()}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f(x)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.],\n",
       "        [4.],\n",
       "        [6.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with this though, the function **accumulates** the adjoints in the ${\\sf .grad}$ attribute of a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment is Not a Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Complicated\" Example\n",
    "\n",
    "$$f(A, c,x,y) = x^\\top \\cdot A\\cdot x+\\cdot \\sin(y)^\\top \\cdot x$$\n",
    "\n",
    "$A$ is a symmetric matrix,  $c$ is a scalar,  $x$ is a vector, $y$ is a vector.\n",
    "\n",
    "Unlike with Autograd, we can backward through the graph and get all the gradients in one go. The ${\\sf backward}$ function backpropagates the VJPs and stores the accumulated adjoint in the leaf nodes of the graph. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(A, x, y):\n",
    "    z = x.T @ A @ x + torch.sin(y).T @ x\n",
    "    z.backward()\n",
    "    return z\n",
    "\n",
    "dtype = torch.float64\n",
    "\n",
    "A = torch.tensor(np.array([[1.,2.],[2.,1.]]), requires_grad=True, dtype = dtype)\n",
    "x = torch.tensor(np.array([[1.,2.]]).T, requires_grad=True, dtype = dtype)\n",
    "y = torch.tensor(np.array([[1.,1.]]).T, requires_grad=True, dtype = dtype)\n",
    "\n",
    "val = f(A,x,y)\n",
    "\n",
    "# mad dictionary skills\n",
    "gradients = {'A': A.grad, 'x': x.grad, 'y': y.grad}\n",
    "\n",
    "for k in gradients.keys():\n",
    "    print(k, ':\\n', gradients[k].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Order Derivatives\n",
    "\n",
    "This is my attempt to make a function that computes the whole function by first creating a function  $g(\\mathbf{x}) := \\nabla f(\\mathbf{x}$. Remember that the Hessian is just $\\mathrm{J}_{\\nabla f} = \\mathrm{J}_g$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def f(x):\n",
    "    return (x[0]**2 + x[0]*x[1])\n",
    "\n",
    "def Hessian(f, x):\n",
    "    g = torch.autograd.grad(f(x), x, create_graph=True)[0]\n",
    "    d = x.shape[0]\n",
    "    rows = [] \n",
    "    for i in range(d):\n",
    "        vec = torch.zeros(d)\n",
    "        vec[i] = 1\n",
    "\n",
    "        g.backward(vec, retain_graph = True) # vector-Jacobian products! (Hessian Rows)\n",
    "        rows.append(x.grad.clone())\n",
    "        x.grad.zero_() # zeroing out the gradients in the leafs\n",
    "        H = torch.stack(rows)\n",
    "    \n",
    "    return H \n",
    "\n",
    "x = torch.tensor([1.,2.], requires_grad=True)\n",
    "\n",
    "Hessian(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "${\\sf PyTorch}$ has a tendency to be faster than ${\\sf Numpy}$ even on a CPU, particularly for large matrices. Compare creating ${\\rm A}$ and ${\\rm B}$ as $d \\times d$ standard normal matrices, and computing their outer product ${\\rm A}{\\rm B}^\\top$. I do it the obvious way, but also using a special function called ${\\sf einsum}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy...\n",
      "81.1 ms ± 2.74 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
      "16.3 ms ± 793 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
      "Pytorch...\n",
      "3.26 ms ± 1.15 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
      "3.69 ms ± 458 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "d = 500\n",
    "\n",
    "print(\"Numpy...\")\n",
    "%timeit -n 5 -r 5 np.einsum('ik, jk ->j', np.random.randn(d,d), np.random.randn(d,d))\n",
    "%timeit -n 5 -r 5 np.random.randn(d,d) @ np.random.randn(d,d).T\n",
    "\n",
    "print(\"Pytorch...\")\n",
    "%timeit -n 5 -r 5 torch.einsum('ik, jk -> j', torch.randn(d,d), torch.randn(d,d))\n",
    "%timeit -n 5 -r 5 torch.randn(d,d) @ torch.randn(d,d). T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration: Calling Autodiff'd Python Functions in ${\\sf R}$\n",
    "\n",
    "One can call Python functions in {\\sf R} using the ${\\sf reticulate}$ package. I will do a quick demo using my ${\\sf{reticdemo.r}}$ and ${\\sf func.py}$ files.\n",
    "\n",
    "My advice for ${\\sf Python}$/${\\sf R}$ interoperability is as follows: \n",
    "\n",
    "(1) Reticulate will convert R objects and will send Python a ${\\sf numpy}$ array --- make sure your function(s) transform them into tensors on input (use ${\\sf torch.tensor}()$)\n",
    "\n",
    "(2) Make sure the function returns a numpy array for each output --- use ${\\sf .numpy()}$ or ${\\sf .data.numpy()}$ on the PyTorch tensors before returning them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "*(1)* Implement ${\\sf f2}$ from the autograd example in Pytorch, but use array assignment,call this function  ${\\sf f2a}$, then do it with a list comprehension, calling it  ${\\sf f2b}$. Compare the speed of the two. \n",
    "\n",
    "\n",
    "*(2)* Using PyTorch/Autograd, create a function that *explicitly* forms the full **Jacobian** for the function $f(x_1, x_2) = (x_1, x_1 x_2)$. You will need an array stacking function. Pass in some numeric values and ensure the answer matches.\n",
    "\n",
    "*(3)* Fully implement the ''complicated'' example in Autograd in a way that only requires one function evaluation to get all gradients, and returns a vector of these gradients. You will need to pack elements going in, and unpack the outputs full gradient vector it outputs - i.e., you have the same output as if you did three forward and backward passes of autodiff. (You will need to make an ${\\sf unpack}$ function. \n",
    "\n",
    "(4) Investigate what the ${\\sf einsum}$ function does (same in ${\\sf numpy}$ and ${\\sf PyTorch}$) - use it to get (1) row sum of a random matrix , (2) column sum of a random matrix, and (3) matrix product of two random matrices. Compare with the direct implementation using standard linear algebra functions. \n",
    "\n",
    "(5) Explore ${\\sf reticulate}$ further and use it to call ${\\sf f1}$ and its gradient from ${\\sf R}$. \n",
    "\n",
    "(6) **Challenge Exercise**: Defining New Primatives in Autograd.\n",
    "\n",
    "${\\sf Autograd}$ does not have build in support to autodiff the **probit** function $\\Phi^{-1}(x), x \\in (0,1)$, so lets implement it! It is simple to show that \n",
    "$$\\frac{{\\rm d}}{{\\rm d}x}\\Phi^{-1}(x) = \\frac{1}{\\phi\\big(\\Phi^{-1}(x)\\big)}.$$\n",
    "\n",
    "Consider the function evaluated elementwise for an input: $\\mathbf{\\Phi}^{-1}(x), \\mathbf{x} \\in (0,1)^n$, so $\\mathbf{\\Phi}^{-1}:(0,1)^n \\rightarrow \\mathbb{R}^n$. \n",
    "\n",
    "Then, \n",
    " $$\\mathrm{J}_{\\mathbf{\\Phi}^{-1}}(\\mathbf{x}) = {\\rm Diag\\left(\\frac{1}{\\mathbf{\\phi}\\big(\\mathbf{\\Phi}^{-1}(\\mathbf{x})\\big)} \\right)},$$\n",
    "\n",
    " and thus \n",
    " \n",
    " $${\\sf vjp}_{\\mathbf{\\Phi^{-1}}}(\\mathbf{x},\\mathbf{r}) = \\mathbf{r}^\\top\\mathrm{J}_{\\Phi^{-1}}(\\mathbf{x}) = [\\mathrm{J}_{\\Phi^{-1}}(\\mathbf{x})^\\top \\mathbf{r}]^\\top = \\left[\\frac{\\mathbf{r}}{\\mathbf{\\phi}\\big(\\mathbf{\\Phi}^{-1}(\\mathbf{x})\\big)}\\right]^\\top.$$\n",
    " \n",
    "Visit the ${\\sf autograd}$ Github page, learn how to create new primitives using the ${\\sf @primitive}$ decorator and ${\\sf defvjp}$ function, and implement these for a new primitive called ${\\sf probit}$. Defining new primitives is similar in ${\\sf PyTorch}$, albeit requiring more Python skills. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "# Numpy Example: Pass by Reference\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def f(y):\n",
    "  y = y**2 \n",
    "\n",
    "x = np.array([5])\n",
    "f(x)\n",
    "\n",
    "print(x) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Example: Broadcasting\n",
    "\n",
    "x = np.array([1,2])\n",
    "y = np.array([[1,2]]).T\n",
    "\n",
    "x = x.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-051d4476b01b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
